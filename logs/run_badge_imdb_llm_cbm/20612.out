Already up to date.
====================
Arguments:
====================
model_name: FacebookAI/roberta-base
max_length: 128
dataset: imdb
seed_size: 50
query_size: 5
rounds: 100
epochs: 5
batch_size: 16
lr: 0.1
alpha_concept: 1.0
concept_l1: 0.0001
device: cuda
seed: 42
cache_dir: None
llm_model: gpt-4o-mini
llm_api_key_env: OPENAI_API_KEY
llm_workers: 8
Latest git commit hash: e57b57611018843682876b0d49d9c72143713b3a

使用数据集: imdb
标签数量: 2
概念数量: 24
标签: ['negative', 'positive']
Seed labeled=50, unlabeled=24950

=== Preprocessing Encoder Features ===
Preprocessing encoder features for 25000 samples...
  Progress: 76.9% (10/13 batches, 15.7s)
Saving preprocessed features to cache/imdb/FacebookAI_roberta-base/train_encoder_features.npz...
Saved 25000 features. Total time: 19.02s
[Time] Train set preprocessing time: 19.02s
Preprocessing encoder features for 25000 samples...
  Progress: 76.9% (10/13 batches, 16.0s)
Saving preprocessed features to cache/imdb/FacebookAI_roberta-base/test_encoder_features.npz...
Saved 25000 features. Total time: 19.17s
[Time] Test set preprocessing time: 19.17s
Created train feature cache dictionary with 25000 entries.
Created test feature cache dictionary with 25000 entries.
LLM annotated concepts for initial 50 samples, with 1 failures.
Begin Iterative Annotation...

=== Round 0/100 ===
[Time] Training: 1.52s
Test accuracy: 50.03%
[Time] Evaluation: 17.32s
[Time] Probe selection (20000 samples): 0.01s
